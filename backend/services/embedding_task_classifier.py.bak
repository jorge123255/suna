"""
Embedding-based task classifier for determining the appropriate model for a given task.
"""

import numpy as np
from typing import Dict, Any, Tuple, List, Optional
import os
import json
import requests
from utils.logger import logger
from utils.config import config

# Task types and their corresponding models
TASK_MODELS = {
    "coding": "qwen2.5-coder:32b-instruct-q8_0",
    "reasoning": "mixtral:8x22b-instruct-v0.1-q4_K_M",
    "creative": "llama3:8b",
    "general": "qwen2.5:32b-instruct-q4_K_M"
}

# Example prompts for each task type
TASK_EXAMPLES = {
    "coding": [
        "Can you help me debug this Python code?",
        "Write a function to calculate Fibonacci numbers",
        "How do I implement a binary search tree in JavaScript?",
        "What's the best way to optimize this SQL query?",
        "Create a React component for a login form",
        "Help me understand this error in my code",
        "How do I use async/await in JavaScript?",
        "Write a unit test for this function",
        "Explain the difference between inheritance and composition",
        "How do I connect to a MongoDB database in Node.js?"
    ],
    "reasoning": [
        "What are the pros and cons of remote work?",
        "Analyze the impact of AI on the job market",
        "Compare and contrast microservices vs monolithic architecture",
        "What are the ethical implications of facial recognition technology?",
        "Explain the concept of diminishing returns",
        "What factors should I consider when making this business decision?",
        "Analyze this research paper and explain its key findings",
        "What are the logical fallacies in this argument?",
        "How might climate change affect global food security?",
        "What are the trade-offs between privacy and security in technology?"
    ],
    "creative": [
        "Write a short story about a time traveler",
        "Create a poem about the ocean",
        "Imagine a world where humans can fly",
        "Design a character for a fantasy novel",
        "Come up with a unique restaurant concept",
        "Write a dialogue between two historical figures",
        "Create a marketing slogan for a new eco-friendly product",
        "Describe an alien civilization unlike anything on Earth",
        "Write a song about overcoming adversity",
        "Design a game concept with unique mechanics"
    ],
    "general": [
        "What's the weather like today?",
        "How do I make pasta?",
        "Tell me about the history of Rome",
        "What movies are playing this weekend?",
        "How tall is Mount Everest?",
        "What's the capital of Australia?",
        "How do I change a flat tire?",
        "What are some good books to read?",
        "Tell me about quantum physics",
        "What's the difference between alligators and crocodiles?"
    ]
}

# Cache for embeddings to avoid recomputing
EMBEDDING_CACHE = {}
TASK_EMBEDDINGS = {}

def cosine_similarity(a: List[float], b: List[float]) -> float:
    """
    Calculate cosine similarity between two vectors.
    
    Args:
        a: First vector
        b: Second vector
        
    Returns:
        Cosine similarity (between -1 and 1)
    """
    a = np.array(a)
    b = np.array(b)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def get_embedding(text: str) -> List[float]:
    """
    Get embedding for a text using a local model or external API.
    
    Args:
        text: Text to get embedding for
        
    Returns:
        Embedding vector
    """
    # Check cache first
    if text in EMBEDDING_CACHE:
        return EMBEDDING_CACHE[text]
    
    try:
        # Option 1: Use Ollama for embeddings (if available)
        try:
            response = requests.post(
                "http://localhost:11434/api/embeddings",
                json={"model": "nomic-embed-text", "prompt": text}
            )
            if response.status_code == 200:
                embedding = response.json().get("embedding", [])
                EMBEDDING_CACHE[text] = embedding
                return embedding
        except Exception as e:
            logger.warning(f"Failed to get embedding from Ollama: {e}")
        
        # Option 2: Use OpenAI's embeddings API (if configured)
        if config.OPENAI_API_KEY:
            import openai
            openai.api_key = config.OPENAI_API_KEY
            
            response = openai.embeddings.create(
                model="text-embedding-3-small",
                input=text
            )
            embedding = response.data[0].embedding
            EMBEDDING_CACHE[text] = embedding
            return embedding
            
        # Option 3: Use a simple fallback method if no embedding service is available
        # This is not ideal but better than failing completely
        words = text.lower().split()
        # Create a very simple embedding (word count based)
        simple_embedding = [0.0] * 100  # 100-dimensional space
        for i, word in enumerate(words[:100]):
            # Hash the word to get a consistent position
            position = hash(word) % 100
            simple_embedding[position] += 1.0 / (i + 1)  # Weight by position
            
        # Normalize
        norm = np.linalg.norm(simple_embedding)
        if norm > 0:
            simple_embedding = [x / norm for x in simple_embedding]
            
        EMBEDDING_CACHE[text] = simple_embedding
        return simple_embedding
            
    except Exception as e:
        logger.error(f"Error getting embedding: {e}")
        # Return a zero vector as fallback
        return [0.0] * 100

def initialize_task_embeddings():
    """
    Initialize task embeddings by averaging example embeddings.
    """
    global TASK_EMBEDDINGS
    
    # Skip if already initialized
    if TASK_EMBEDDINGS:
        return
        
    for task_type, examples in TASK_EXAMPLES.items():
        # Get embeddings for all examples
        embeddings = [get_embedding(example) for example in examples]
        
        # Average the embeddings
        if embeddings:
            avg_embedding = np.mean(embeddings, axis=0).tolist()
            TASK_EMBEDDINGS[task_type] = avg_embedding
            logger.info(f"Initialized embedding for task type: {task_type}")

def classify_task_with_embeddings(prompt: str) -> Tuple[str, float]:
    """
    Classify a task based on the prompt using embeddings.
    
    Args:
        prompt: The user's prompt
        
    Returns:
        A tuple of (task_type, confidence)
    """
    # Initialize task embeddings if not already done
    if not TASK_EMBEDDINGS:
        initialize_task_embeddings()
    
    # Get embedding for the prompt
    prompt_embedding = get_embedding(prompt)
    
    # Calculate similarity with each task type
    similarities = {}
    for task_type, task_embedding in TASK_EMBEDDINGS.items():
        similarity = cosine_similarity(prompt_embedding, task_embedding)
        similarities[task_type] = similarity
    
    logger.debug(f"Task similarities: {similarities}")
    
    # Find the task type with highest similarity
    max_similarity = -1
    best_task_type = "general"  # Default
    
    for task_type, similarity in similarities.items():
        if similarity > max_similarity:
            max_similarity = similarity
            best_task_type = task_type
    
    # Convert similarity (-1 to 1) to confidence (0 to 1)
    confidence = (max_similarity + 1) / 2
    
    logger.info(f"Classified task as {best_task_type} with confidence {confidence:.2f}")
    return best_task_type, confidence

def get_model_for_task(prompt: str) -> str:
    """
    Get the appropriate model for a given task using embedding-based classification.
    
    Args:
        prompt: The user's prompt
        
    Returns:
        The name of the model to use
    """
    task_type, confidence = classify_task_with_embeddings(prompt)
    
    # Use the task-specific model if confidence is high enough
    if confidence >= 0.6:  # Higher threshold for embedding-based approach
        return TASK_MODELS[task_type]
    
    # Fall back to general model if confidence is low
    return TASK_MODELS["general"]

def save_embeddings_cache(file_path: str = "embedding_cache.json"):
    """
    Save the embedding cache to a file.
    
    Args:
        file_path: Path to save the cache to
    """
    try:
        with open(file_path, "w") as f:
            json.dump(EMBEDDING_CACHE, f)
        logger.info(f"Saved embedding cache to {file_path}")
    except Exception as e:
        logger.error(f"Failed to save embedding cache: {e}")

def load_embeddings_cache(file_path: str = "embedding_cache.json"):
    """
    Load the embedding cache from a file.
    
    Args:
        file_path: Path to load the cache from
    """
    global EMBEDDING_CACHE
    
    try:
        if os.path.exists(file_path):
            with open(file_path, "r") as f:
                EMBEDDING_CACHE = json.load(f)
            logger.info(f"Loaded embedding cache from {file_path} with {len(EMBEDDING_CACHE)} entries")
    except Exception as e:
        logger.error(f"Failed to load embedding cache: {e}")

def get_available_task_models() -> List[Dict[str, Any]]:
    """
    Get a list of available task-specific models.
    
    Returns:
        A list of model information
    """
    return [
        {
            "task_type": task_type,
            "model_name": model_name,
            "description": f"Optimized for {task_type} tasks"
        }
        for task_type, model_name in TASK_MODELS.items()
    ]

# Initialize embeddings when module is loaded
try:
    initialize_task_embeddings()
except Exception as e:
    logger.error(f"Failed to initialize task embeddings: {e}")
